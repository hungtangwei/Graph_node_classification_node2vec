{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talbe of contents\n",
    "\n",
    "* [Taks 2: Node Classification in Graphs](#task2)\n",
    "    * [Introduction](#introduction2)\n",
    "    * [Import libraries](#libraries2)\n",
    "    * [Data preprocessing](#dataprep)\n",
    "    * [Text Embedding](#text)\n",
    "    * [Node Embedding](#node)\n",
    "    * [SVM Classifer Model(with node embedding) ](#svc1)\n",
    "    * [SVM Classifer Model(with node embedding + text embedding)](#svc2)\n",
    "    * [Compare model and result conclusion](#conclusion2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Node Classification in Graphs\n",
    "<a id=\"task2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "<a id=\"introduction2\"></a>\n",
    "\n",
    "In the task 2, I will perform the node classification by using the graph dataset of citation network. In this dataset, each node is paper, and edge indicates the relationship between two paper. Also, we have the title information of each paper. Then we will do the text embedding and node embedding. After finish embedding, I will use the SVM classifier method to build two classifier, one is only contain node embedding, and another is contain node embedding and text embedding. Finally, we will evaluate the result.\n",
    "\n",
    "\n",
    "The steps in Node Classification in graphs are:\n",
    "1. Get the nodes (paper) from docs.txt\n",
    "2. Get the edges (between paper and paper) from adjedges.txt\n",
    "3. Add the nodes and edges into graph\n",
    "4. Do the text embedding\n",
    "5. Do the node embedding\n",
    "6. Combine the node embedding with text embedding \n",
    "7. Split the dataset into train dataset and test dataset (with seed=0)\n",
    "6. Build the SVM classifer(with node embedding) and VM classifer(with node embedding+ text embedding).\n",
    "7. Fit the data\n",
    "8. Evaluate the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "<a id=\"libraries2\"></a>\n",
    "In this part, I will import some libraries for the following task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install node2vec\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy as sp\n",
    "import scipy.sparse.linalg as linalg\n",
    "import scipy.cluster.hierarchy as hr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.utils as utils\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.svm as svm\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "<a id=\"dataprep\"></a>\n",
    "\n",
    "The steps in data preprocessing are:\n",
    "1. Get the nodes (paper) and title from docs.txt\n",
    "2. Get the edges (between paper and paper) from adjedges.txt\n",
    "3. Add the nodes and edges into graph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# get the node and id title\n",
    "with open('docs.txt') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "id_link={}\n",
    "id_title={}\n",
    "for i in content:\n",
    "    x=re.match(r\"(^\\d*)\", i)\n",
    "    x1=re.match(r\"^\\d* (.*)\",i)\n",
    "    title=x1.group(1)\n",
    "    book_id=x.group()\n",
    "    id_link[book_id]=[]\n",
    "    id_title[book_id]=title\n",
    "\n",
    "# get the the edge between nodes\n",
    "with open('adjedges.txt') as f:\n",
    "    content2 = f.readlines()\n",
    "for i in content2:\n",
    "    links=i.split()\n",
    "    if len(links)>1:\n",
    "        book_id=links[0]\n",
    "        if book_id in id_link.keys():\n",
    "            orig_link=id_link[book_id]\n",
    "            in_id=[j for j in links[1:]]\n",
    "            final_link=orig_link+in_id\n",
    "            id_link[book_id]=final_link \n",
    "\n",
    "# get the label of nodes\n",
    "with open('labels.txt') as f:\n",
    "    content3 = f.readlines()\n",
    "final_id=[]\n",
    "final_label=[]\n",
    "for i in content3:\n",
    "    links=i.split()\n",
    "    final_id.append(links[0])\n",
    "    final_label.append(links[1])\n",
    "\n",
    "subject = pd.DataFrame({'id': final_id, 'label':final_label})\n",
    "subject = subject.set_index(\"id\")\n",
    "final_subject = subject[\"label\"]\n",
    "\n",
    "\n",
    "# Create an empty graph with no nodes and no edges\n",
    "g = nx.Graph()\n",
    "\n",
    "# add the nodes and edges to the graph\n",
    "for key, value in id_link.items():\n",
    "    g.add_node(key)\n",
    "    for i in value:\n",
    "        g.add_edge(key,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding<a id=\"text\"></a>\n",
    "\n",
    "In this step, I will use the tf-idf to perform the text embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDocs=list(id_title.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a LemmaTokenizer to vectorize the text\n",
    "class LemmaTokenizerSpacy(object):        \n",
    "    def __call__(self,doc):\n",
    "        trydoc = nlp(doc)\n",
    "        return [token.lemma_ for token in trydoc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of stop words\n",
    "stopwords_list = list(en_stop)\n",
    "add_stopwords=[\"'\", '-PRON-', 'd', 'm', 'regard', 'use', 've','-pron-']\n",
    "stopwords_list.extend(add_stopwords)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# get the vectorizer\n",
    "vectorizer=TfidfVectorizer(analyzer='word',input='content',\n",
    "                           lowercase=True,\n",
    "                           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                           stop_words=stopwords_list,\n",
    "                           min_df=3,\n",
    "                           ngram_range=(1,2),\n",
    "                           tokenizer=LemmaTokenizerSpacy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_sparse=vectorizer.fit_transform(trainDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding=text_embedding_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Embedding\n",
    "<a id=\"node\"></a>\n",
    "In step of Node embedding, I will use the Node2Vec method to embed the node. In Node2Vec, when developing a random walk, there is a certain probability to go back to the previous node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 36928/36928 [00:08<00:00, 4162.30it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 150/150 [1:38:45<00:00, 39.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from node2vec import Node2Vec\n",
    "# pre-compute the probabilities and generate walks :\n",
    "node2vec = Node2Vec(g, dimensions=64, walk_length=30, num_walks=150, workers=1)\n",
    "# embed the nodes\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "model.save('node2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "#model = Word2Vec.load(\"node2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of test node IDs\n",
    "node_ids = model.wv.index2word\n",
    "# numpy.ndarray of size number of nodes*embeddings dimensionality \n",
    "node_embeddings_orig = model.wv.vectors  \n",
    "# set the filter_indices to find each test node in node_ids's index \n",
    "filter_indices=[]\n",
    "for i in final_id:\n",
    "    filter_indices.append(node_ids.index(i))\n",
    "\n",
    "# get the final embedding\n",
    "node_embeddings=np.take(node_embeddings_orig, filter_indices, 0)\n",
    "\n",
    "# list of test node IDs\n",
    "final_node_ids=[]\n",
    "for i in filter_indices:\n",
    "    final_node_ids.append(node_ids[i])\n",
    "\n",
    "node_targets = final_subject[[node_id for node_id in final_node_ids]]\n",
    "\n",
    "# combine the text embedding and node embedding together\n",
    "text_node_embedding=np.concatenate((node_embeddings,text_embedding),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifer Model ( With node embedding)\n",
    "<a id=\"svc1\"></a>\n",
    "In this step, I will build the node classifer by using the SVM model with node embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X as input features\n",
    "X = node_embeddings\n",
    "# Set y as corresponding target values\n",
    "y = np.array(node_targets)\n",
    "\n",
    "# Split the data to train data set and test data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tangwei/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svc=LinearSVC()\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4031  155   54  384    2]\n",
      " [ 231 2476  156  274    4]\n",
      " [2800  281  223  285    0]\n",
      " [ 495  259   52 1366    1]\n",
      " [1155   81   13  198    0]]\n",
      "Accuracy: 0.5405982905982906\n",
      "Macro Precision: 0.44334765267987386\n",
      "Macro Recall: 0.47008429371822613\n",
      "Macro F1 score:0.41438456893266806\n",
      "MCC:0.4208782507548116\n"
     ]
    }
   ],
   "source": [
    "y_pred_LSVC = svc.predict(X_test)\n",
    "confusion_matrix_LSVC=confusion_matrix(y_test, y_pred_LSVC)\n",
    "recall_LSVC=recall_score(y_test, y_pred_LSVC,average='macro')\n",
    "precision_LSVC=precision_score(y_test, y_pred_LSVC,average='macro')\n",
    "f1score_LSVC=f1_score(y_test, y_pred_LSVC,average='macro')\n",
    "accuracy_LSVC=accuracy_score(y_test, y_pred_LSVC)\n",
    "matthews_LSVC = matthews_corrcoef(y_test, y_pred_LSVC) \n",
    "print(confusion_matrix_LSVC)\n",
    "print('Accuracy: '+ str(accuracy_LSVC))\n",
    "print('Macro Precision: '+ str(precision_LSVC))\n",
    "print('Macro Recall: '+ str(recall_LSVC))\n",
    "print('Macro F1 score:'+ str(f1score_LSVC))\n",
    "print('MCC:'+ str(matthews_LSVC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifer Model ( With node embedding + text embedding)\n",
    "<a id=\"svc2\"></a>\n",
    "In this step, I will build the node classifer by using the SVM model with node embedding + text embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X as input features\n",
    "X = text_node_embedding\n",
    "# Set y as corresponding target values\n",
    "y = np.array(node_targets)\n",
    "\n",
    "# Split the data to train data set and test data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tangwei/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#rbf_svc = svm.SVC(kernel='rbf')\n",
    "#rbf_svc.fit(X_train, y_train)\n",
    "svc=LinearSVC()\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3953   70  298  121  184]\n",
      " [  61 2818  134  112   16]\n",
      " [ 591  202 2604  139   53]\n",
      " [ 118  140  105 1798   12]\n",
      " [ 540   54  106   72  675]]\n",
      "Accuracy: 0.7911324786324786\n",
      "Macro Precision: 0.7862422202017484\n",
      "Macro Recall: 0.7542289277999005\n",
      "Macro F1 score:0.763716229268779\n",
      "MCC:0.7289803631256759\n"
     ]
    }
   ],
   "source": [
    "#y_pred_LSVC = rbf_svc.predict(X_test)\n",
    "y_pred_LSVC = svc.predict(X_test)\n",
    "confusion_matrix_LSVC=confusion_matrix(y_test, y_pred_LSVC)\n",
    "recall_LSVC=recall_score(y_test, y_pred_LSVC,average='macro')\n",
    "precision_LSVC=precision_score(y_test, y_pred_LSVC,average='macro')\n",
    "f1score_LSVC=f1_score(y_test, y_pred_LSVC,average='macro')\n",
    "accuracy_LSVC=accuracy_score(y_test, y_pred_LSVC)\n",
    "matthews_LSVC = matthews_corrcoef(y_test, y_pred_LSVC) \n",
    "print(confusion_matrix_LSVC)\n",
    "print('Accuracy: '+ str(accuracy_LSVC))\n",
    "print('Macro Precision: '+ str(precision_LSVC))\n",
    "print('Macro Recall: '+ str(recall_LSVC))\n",
    "print('Macro F1 score:'+ str(f1score_LSVC))\n",
    "print('MCC:'+ str(matthews_LSVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_LSVC=accuracy_score(y_test, y_pred_LSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model and result conclusion\n",
    "<a id=\"conclusion2\"></a>\n",
    "\n",
    "SVM Classifer Model( With node embedding):\n",
    "\n",
    "|              | Pred=0 | Pred=1 |Pred=2 |Pred=3 |Pred=4 |\n",
    "| -----------  | ----------- | ----------- |----------- |----------- |----------- |\n",
    "| **Actual=0**  | 4060      |   151  |15  |400  |0  |\n",
    "| **Actual=1** | 256        |   2527 |101 |256 |1 |\n",
    "| **Actual=2** | 2800        |   275 |205 |308 |1 |\n",
    "| **Actual=3** | 559        |   224 |34 |1355 |1 |\n",
    "| **Actual=4** | 1168        |   75 |8 |192 |4 |\n",
    "\n",
    "Accuracy: 0.544\n",
    "\n",
    "Macro Precision: 0.582\n",
    "\n",
    "Macro Recall: 0.473\n",
    "\n",
    "Macro F1 score: 0.416\n",
    "\n",
    "MCC: 0.429\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "SVM Classifer Model ( With node embedding + text embedding):\n",
    "\n",
    "|              | Pred=0 | Pred=1 |Pred=2 |Pred=3 |Pred=4 |\n",
    "| -----------  | ----------- | ----------- |----------- |----------- |----------- |\n",
    "| **Actual=0**  | 3953     |   70  |298  |121  |184  |\n",
    "| **Actual=1** | 61        |   2818 |134 |112 |16 |\n",
    "| **Actual=2** | 591        |   202 |2604 |139 |53 |\n",
    "| **Actual=3** | 118        |   140 |105 |1798 |12 |\n",
    "| **Actual=4** | 540        |  54 |106 |72 |675 |\n",
    "\n",
    "Accuracy: 0.791\n",
    "\n",
    "Macro Precision: 0.786\n",
    "\n",
    "Macro Recall: 0.754\n",
    "\n",
    "Macro F1 score: 0.763\n",
    "\n",
    "MCC: 0.728\n",
    "\n",
    "\n",
    "------------\n",
    "According to the result:\n",
    "1.\tThe SVM classifer model **(with node embedding + text embedding)** has higher accuracy than the SVM classifer model(with node embedding)\n",
    "2.\tBeside, The SVM classifer model **(with node embedding + text embedding)** has higher precision, recall, F1 score than logistic regression model. This means the the SVM classifer model **(with node embedding + text embedding)** is most suitable in this case.\n",
    "3.\tThe reason that the SVM classifer model **(with node embedding)** has extremely sparse network structure in node embedding. Therefore, this model will not do the very well when extracting the features and cause the bad performance in accuracy.\n",
    "4. In the future, if we want to chase higher accuracy maybe we could use the Graph Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
